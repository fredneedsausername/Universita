\documentclass{article}

\usepackage{amsmath}
\usepackage[italian]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{xcolor}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

% This creates a new tcolorbox for definitions
\newtcbtheorem{definition}{Definizione} % {box name}{display name}
{
    colback=magenta!10!white,
    colframe=magenta!80!black,
    fonttitle=\bfseries
}
{def} % This is a prefix for the label, so you can reference it like \ref{def:vector_space}

\newtcbtheorem{theorem}{Teorema} % {box name}{display name}
{
    colback=blue!10!white,
    colframe=blue!80!black,
    fonttitle=\bfseries
}
{thm}

\title{Algebra Lineare}
\author{Federico Veronesi}

\begin{document}
\maketitle

{
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\section{Vettori}

\begin{definition}{Vettore}{vettore}
    Un vettore di dimensione $n$ su un campo $\mathbb{K}$ è una $n$-upla ordinata di elementi $x_1, x_2, \dots, x_n$ appartenenti a $\mathbb{K}$. Si rappresenta comunemente come un vettore colonna:
    \[ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \]
    Ad esempio, $\begin{pmatrix} 2 \\ -3.5 \\ \pi \end{pmatrix}$ è un vettore di dimensione $3$ sul campo $\mathbb{R}$.
\end{definition}

\begin{definition}{Spazio Vettoriale}{spazio_vettoriale}
    Uno \textbf{spazio vettoriale} su un campo $\mathbb{K}$ è un insieme non vuoto $V$ di \hyperref[def:vettore]{vettori}, su cui sono definite due operazioni:
    \begin{enumerate}
        \item \textbf{Addizione fra vettori:} $V \times V \to V$, che associa a due vettori $u, v \in V$ un terzo vettore $u+v \in V$.
        \item \textbf{Moltiplicazione per scalare:} $\mathbb{K} \times V \to V$, che associa a uno scalare $\lambda \in \mathbb{K}$ e un vettore $v \in V$ un altro vettore $\lambda v \in V$.
    \end{enumerate}
    Queste operazioni devono soddisfare determinate proprietà (associatività, commutatività, esistenza dell'elemento neutro e dell'inverso, distributività).
\end{definition}

Alcuni esempi di spazi vettoriali sono i seguenti:
\begin{itemize}
    \item $\mathbb{R}[x]$: L'insieme di tutti i polinomi a coefficienti reali.
    \item $\mathbb{R}[x]_{\le n}$: L'insieme dei polinomi di grado al più $n$.
    \item $C(\mathbb{R}, \mathbb{R})$: L'insieme delle funzioni continue da $\mathbb{R}$ in $\mathbb{R}$.
    \item $M(m, n, \mathbb{K})$: L'insieme delle matrici $m \times n$ a coefficienti nel campo $\mathbb{K}$.
\end{itemize}

\subsection{Sottospazi Vettoriali}

\begin{definition}{Sottospazio Vettoriale}{sottospazio_vettoriale}
    Un sottoinsieme non vuoto $W \subseteq V$ è un sottospazio vettoriale di $V$ se $W$ è a sua volta uno \hyperref[def:spazio_vettoriale]{spazio vettoriale} rispetto alle operazioni definite in $V$. Ciò è vero se e solo se $W$ è chiuso rispetto alle due operazioni:
    \begin{itemize}
        \item $\forall w_1, w_2 \in W \implies w_1 + w_2 \in W$
        \item $\forall \lambda \in \mathbb{K}, \forall w \in W \implies \lambda w \in W$
    \end{itemize}
    Ne consegue che ogni sottospazio vettoriale deve contenere il vettore nullo.
\end{definition}

\subsection{Combinazioni Lineari}

\begin{definition}{Combinazione Lineare}{combinazione_lineare}
    Dati $k$ \hyperref[def:vettore]{vettori} $v_1, v_2, \dots, v_k$ di uno spazio vettoriale $V$ e $k$ scalari $\lambda_1, \lambda_2, \dots, \lambda_k$ del campo $\mathbb{K}$, si definisce \textbf{combinazione lineare} il vettore $v$ ottenuto come:
    \[ v = \lambda_1 v_1 + \lambda_2 v_2 + \dots + \lambda_k v_k \]
    L'insieme di tutte le possibili combinazioni lineari di un gruppo di vettori è definito \hyperref[def:funzione_span]{SPAN}.
\end{definition}

\begin{definition}{Funzione SPAN (o Sottospazio Generato)}{funzione_span}
    La funzione SPAN (o \textit{chiusura lineare}) accetta un insieme di \hyperref[def:vettore]{vettori} $S = \{v_1, \dots, v_k\}$ e restituisce il sottospazio vettoriale $W$ contenente tutte le loro possibili combinazioni lineari. Tale sottospazio è detto "sottospazio generato da $S$".
    
    In notazione:
    \[ \text{SPAN}(v_1, \dots, v_k) = \{ \lambda_1 v_1 + \dots + \lambda_k v_k \mid \lambda_1, \dots, \lambda_k \in \mathbb{K} \} \]
    \textbf{Esempio con due vettori in $\mathbb{R}^2$:}
    \[ \text{SPAN}\left(\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}\right) = \left\{ \lambda\begin{pmatrix} 1 \\ 0 \end{pmatrix} + \mu\begin{pmatrix} 0 \\ 1 \end{pmatrix} : \lambda, \mu \in \mathbb{R}\right\} = \mathbb{R}^2 \]
\end{definition}

\begin{definition}{Dipendenza e Indipendenza Lineare}{dipendenza_lineare}
    Un insieme di vettori $\{v_1, \dots, v_k\}$ si dice \textbf{linearmente indipendente} se l'unica loro combinazione lineare che risulta nel vettore nullo è quella con tutti i coefficienti nulli:
    \[ \lambda_1 v_1 + \dots + \lambda_k v_k = \mathbf{0} \iff \lambda_1 = \lambda_2 = \dots = \lambda_k = 0 \]
    In caso contrario, i vettori si dicono \textbf{linearmente dipendenti}. Questo equivale a dire che almeno uno di essi può essere espresso come \hyperref[def:combinazione_lineare]{combinazione lineare} degli altri.
\end{definition}

\begin{definition}{Base}{base}
    Una \textbf{base} di uno \hyperref[def:spazio_vettoriale]{spazio vettoriale} $V$ è un insieme di \hyperref[def:vettore]{vettori} $\{v_1, \dots, v_n\}$ che soddisfa due condizioni:
    \begin{enumerate}
        \item I vettori sono \hyperref[def:dipendenza_lineare]{linearmente indipendenti}.
        \item I vettori generano l'intero spazio, ovvero $\text{SPAN}(v_1, \dots, v_n) = V$.
    \end{enumerate}
    Ogni vettore dello spazio può essere scritto in modo \textbf{unico} come combinazione lineare dei vettori di una base.
\end{definition}

\begin{definition}{Dimensione di Uno Spazio Vettoriale}{dimensione_spazio_vettoriale}
    La \textbf{dimensione} di uno spazio vettoriale è il numero di vettori che compongono una sua qualsiasi \hyperref[def:base]{base}.
\end{definition}

\begin{theorem}{Teorema Della Dimensione}{}
    Se uno spazio vettoriale $V$ ammette una base finita, allora tutte le sue basi hanno lo stesso numero di vettori. Tale numero è la dimensione di $V$.
\end{theorem}

\subsection{Estrarre una Base da un Insieme di Vettori}

Per trovare una base del sottospazio generato da un insieme di vettori $\{v_1, \dots, v_k\}$, si può utilizzare l'algoritmo di eliminazione di Gauss. L'obiettivo è eliminare i vettori linearmente dipendenti (ridondanti).

\paragraph{Algoritmo Operativo:}
\begin{enumerate}
    \item Si dispongono i vettori come colonne di una matrice $A$.
    \item Si riduce la matrice a una forma a gradini (o "a scala") per colonne, utilizzando operazioni elementari di colonna (scambiare due colonne, moltiplicare una colonna per uno scalare non nullo, sommare a una colonna un multiplo di un'altra).
    \item Le colonne non nulle della matrice ridotta formano una base per il sottospazio generato. Il loro numero corrisponde alla dimensione del sottospazio.
\end{enumerate}

\paragraph{Esempio:} Trovare una base per il sottospazio generato da:
\[ S = \left\{ \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} -1 \\ 1 \\ -2 \end{pmatrix} \right\} \]
Costruiamo la matrice e applichiamo le operazioni di colonna:
\[
A = \begin{pmatrix}
1 & 2 & -1 \\
0 & 1 & 1 \\
1 & 1 & -2
\end{pmatrix}
\xrightarrow{\substack{C_2 \to C_2 - 2C_1 \\ C_3 \to C_3 + C_1}}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1 \\
1 & -1 & -1
\end{pmatrix}
\xrightarrow{C_3 \to C_3 - C_2}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & -1 & 0
\end{pmatrix}
\]
La terza colonna è diventata nulla, indicando che il terzo vettore era linearmente dipendente dai primi due. Le prime due colonne della matrice ridotta (o, meglio, i vettori originali corrispondenti) sono linearmente indipendenti e formano una base per il sottospazio.
\[ \text{Base} = \left\{ \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \\ 1 \end{pmatrix} \right\} \]
La forma a scalini per colonne garantisce l'indipendenza lineare perché ogni vettore "pivot" ha un elemento non nullo in una posizione dove i vettori precedenti hanno zero.

\section{Matrici}

\begin{definition}{Matrice}{matrice}
    Una matrice $m \times n$ (si legge "m per n") su un campo $\mathbb{K}$ è una tabella rettangolare di elementi di $\mathbb{K}$ disposti in $m$ righe e $n$ colonne.
    \[ \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix} \]
    Esempio di matrice $3 \times 2$ sul campo $\mathbb{N}$: $\begin{pmatrix} 1 & 4 \\ 3 & 7 \\ 2 & 13 \end{pmatrix}$
\end{definition}

\begin{definition}{Traccia}{traccia}
    La traccia di una \hyperref[def:matrice_quadrata]{matrice quadrata} è la somma degli elementi sulla sua diagonale principale. Ad esempio, la traccia di $\begin{pmatrix} 6 & 2 \\ 5 & 7 \end{pmatrix}$ è $6+7=13$.
\end{definition}

\subsection{Tipologie di Matrici}

\begin{definition}{Matrice Quadrata}{matrice_quadrata}
Una matrice si dice quadrata se il numero di righe è uguale al numero di colonne ($m=n$).
\end{definition}

\begin{definition}{Matrice Triangolare Superiore}{matrice_triangolare_superiore}
Una matrice quadrata si dice triangolare superiore se tutti gli elementi al di sotto della diagonale principale sono nulli ($a_{ij} = 0$ per $i>j$).
\end{definition}

\begin{definition}{Matrice Triangolare Inferiore}{matrice_triangolare_inferiore}
Una matrice quadrata si dice triangolare inferiore se tutti gli elementi al di sopra della diagonale principale sono nulli ($a_{ij} = 0$ per $i<j$).
\end{definition}

\begin{definition}{Matrice Diagonale}{matrice_diagonale}
Una matrice quadrata si dice diagonale se tutti gli elementi al di fuori della diagonale principale sono nulli ($a_{ij} = 0$ per $i \neq j$).
\end{definition}

\begin{definition}{Matrice Simmetrica}{matrice_simmetrica}
Una matrice quadrata $A$ si dice simmetrica se è uguale alla sua trasposta ($A = A^T$), ovvero se $a_{ij} = a_{ji}$ per ogni $i,j$.
\end{definition}

\begin{definition}{Matrice Antisimmetrica}{matrice_anti_simmetrica}
Una matrice quadrata $A$ si dice antisimmetrica (o emisimmetrica) se è uguale all'opposto della sua trasposta ($A = -A^T$), ovvero se $a_{ij} = -a_{ji}$ per ogni $i,j$. Questo implica che gli elementi sulla diagonale principale sono nulli.
\end{definition}

\subsection{Isomorfismi}

\begin{definition}{Isomorfismo}{isomorfismo}
    Un isomorfismo è un'applicazione lineare e biiettiva tra due \hyperref[def:spazio_vettoriale]{spazi vettoriali} $V$ e $W$ (definiti sullo stesso campo). Due spazi vettoriali sono isomorfi se e solo se hanno la stessa \hyperref[def:dimensione_spazio_vettoriale]{dimensione}.
\end{definition}

Un esempio classico di isomorfismo è quello tra lo spazio dei polinomi $\mathbb{R}[x]_{\le n}$ e lo spazio vettoriale $\mathbb{R}^{n+1}$. L'isomorfismo associa a ogni polinomio il vettore dei suoi coefficienti:
\[ a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \quad \longleftrightarrow \quad \begin{pmatrix}
    a_0 \\
    a_1 \\
    a_2 \\
    \vdots \\
    a_n
\end{pmatrix} \]

\end{document}