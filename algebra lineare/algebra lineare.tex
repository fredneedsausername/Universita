\documentclass{article}

\usepackage{amsmath}
\usepackage[italian]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{xcolor}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

% This creates a new tcolorbox for definitions
\newtcbtheorem{definition}{Definizione} % {box name}{display name}
{
    colback=magenta!10!white,
    colframe=magenta!80!black,
    fonttitle=\bfseries
}
{def} % This is a prefix for the label, so you can reference it like \ref{def:vector_space}

\newtcbtheorem{theorem}{Teorema} % {box name}{display name}
{
    colback=blue!10!white,
    colframe=blue!80!black,
    fonttitle=\bfseries
}
{thm}

\newtcbtheorem{exercise}{Esercizio} % {box name}{display name}
{
    colback=green!10!white,
    colframe=green!80!black,
    fonttitle=\bfseries
}
{exc}

\title{Algebra Lineare}
\author{Federico Veronesi}

\begin{document}
\maketitle

{
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\section{Vettori}

\begin{definition}{Vettore}{vettore}
    Un vettore di campo $\mathbb{K}$ e dimensione $n$ è un insieme di elementi $a_1, a_2, \dots , a_n$, ciascuno di campo $\mathbb{K}$. In notazione: $\begin{pmatrix} x1 \\ \vdots \\ x_n \end{pmatrix}$ come ad esempio $\begin{pmatrix} 2 \\ -3.5 \\ \pi \end{pmatrix}$ di dimensione $3$ e campo $\mathbb{R}$
\end{definition}

\begin{definition}{Spazio Vettoriale}{spazio_vettoriale}
    Uno \textbf{spazio vettoriale} su un campo $\mathbb{K}$ è un insieme di \hyperref[def:vettore]{vettori} $V$ che definisce due operazioni:
    \begin{enumerate}
        \item Addizione fra vettori: $V + V \to V$
        \item Moltiplicazione scalare: $\mathbb{K} \times V \to V$
    \end{enumerate}
\end{definition}

Alcuni esempi di spazi vettoriali sono i seguenti:
\begin{itemize}
    \item $\mathbb{R} [x]$ L'insieme dei polinomi
    \item $\mathbb{R} [x]^{\le n}$ L'insieme dei polinomi di grado $\le n$
    \item $C(\mathbb{R}, \mathbb{R})$ Le funzioni continue da $\mathbb{R}$ in $\mathbb{R}$
    \item $M(m, n, \mathbb{K})$ L'insieme delle matrici $m \times n$ a coefficiente di campo $\mathbb{K}$
\end{itemize}

\subsection{Sottospazi Vettoriali}

\begin{definition}{Sottospazio Vettoriale}{sottospazio_vettoriale}
    Un sottospazio vettoriale è uno \hyperref[def:spazio_vettoriale]{spazio vettoriale} $W$ incluso in uno spazio vettoriale $V$. Moltiplicazione scalare di un vettore in $W$ e somma fra vettori di $W$ sono operazioni che ritornano un vettore incluso in $W$. Ne consegue che ogni sottospazio vettoriale include l'elemento neutro dello spazio vettoriale.
\end{definition}

\subsection{Combinazioni Lineari}

\begin{definition}{Combinazione Lineare}{combinazione_lineare}
    \begin{itemize}
        \item La combinazione lineare è il prodotto di un \hyperref[def:vettore]{vettore} per uno scalare.
        \item Le combinazioni lineari di un vettore sono l'insieme di tutti i vettori risultanti dal prodotto di quel vettore e uno scalare $\lambda$.
        \item Le combinazioni lineari di più vettori insieme sono l'insieme di tutte le somme vettoriali di tutte le combinazioni di combinazioni lineari di ciascuno dei vettori. In notazione: \it{guardare definizione di \hyperref[def:funzione_span]{SPAN}}
    \end{itemize}
\end{definition}

\begin{definition}{Funzione SPAN}{funzione_span}
    La funzione SPAN prende in ingresso un insieme di \hyperref[def:vettore]{vettori} e ritorna l'insieme delle loro combinazioni lineari.
    
    In notazione: \begin{itemize}
        \item \[SPAN(\begin{pmatrix}
            x_1 \\
            \vdots \\
            x_n
            \end{pmatrix}) = \{\begin{pmatrix}
            \lambda x_1 \\
            \vdots \\
            \lambda x_n
            \end{pmatrix} : \lambda \in \mathbb{R}\}\]
        \item \[SPAN(\begin{pmatrix}
            1 \\
            0
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            1
            \end{pmatrix}) = \{ \lambda\begin{pmatrix}
            1 \\
            0
            \end{pmatrix} + r\begin{pmatrix}
            0 \\
            1
            \end{pmatrix} : \lambda, r \in \mathbb{R}\}\]
    \end{itemize}
\end{definition}

\begin{definition}{Dipendenza Lineare}{dipendenza_lineare}
    Dei \hyperref[def:vettore]{vettori} sono linearmente indipendenti se nessuno di loro può essere espresso come \hyperref[def:combinazione_lineare]{combinazione lineare} degli altri.
\end{definition}

\begin{definition}{Base}{base}
    Ognuna delle basi di un insieme di \hyperref[def:vettore]{vettori} è un suo sottoinsieme di vettori \hyperref[def:dipendenza_lineare]{linearmente indipendenti} e la cui combinazione lineare genera l'insieme di partenza stesso

    "Da una base, con un vettore di coefficienti, posso ricavare qualsiasi vettore"
\end{definition}

\begin{exercise}{Basi}{}
    Dalla base $(\begin{pmatrix}
            1 \\
            2 \\
            3
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            1 \\
            4
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            0 \\
            7
            \end{pmatrix})$ o da un'altra comunque simile inventata da te, con gli zero nei vettori non sostutuiti con numeri diversi da zero, ricava $\begin{pmatrix}
            4 \\
            6 \\
            9
            \end{pmatrix}$
\end{exercise}

\begin{exercise}{Basi}{}
    Dimostra che
    $(\begin{pmatrix}
        1 \\
        1 
    \end{pmatrix}, \begin{pmatrix}
        2 \\
        1 
    \end{pmatrix})$ è una base di $\mathbb(R)^2$
\end{exercise}

\begin{exercise}{Teorema Della Dimensione}{dimensione}
    Se uno spazio vettoriale $V$ ammette una base finita, allora tutte le sue basi hanno lo stesso numero di vettori, pari all'ordine dello spazio vettoriale.
\end{exercise}

\begin{theorem}{Teorema \dots}{}
    \[SPAN(v_1, v_2, v_3) = SPAN(v_1, v_2, v_3 + SPAN(v_1, v_2))\]  
\end{theorem}

\section{Matrici}

\begin{definition}{Matrice}{matrice}
    Una matrice è un \hyperref[def:vettore]{vettore} bidimensionale $m \times n$. In notazione: $\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}$ Esempio di matrice di campo $\mathbb{N}$ e di dimensione $2 \times 3$: $\begin{pmatrix}
    1 & 4 \\
    3 & 7 \\
    2 & 13
    \end{pmatrix}$
\end{definition}

\begin{definition}{Traccia}{traccia}
    La traccia di una \hyperref[def:matrice]{matrice} è la somma degli elementi sulla sua diagonale principale. Ad esempio, la traccia di $\begin{pmatrix} 6 & 2 \\ 5 & 7 \end{pmatrix}$ è 13.
\end{definition}

\subsection{Tipologie di Matrici}

\subsubsection*{Matrice Quadrata}
Una matrice si dice quadrata se il numero di righe è uguale al numero di colonne ($m=n$).
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:} Matrice quadrata $3 \times 3$:
\[ \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{pmatrix} \]

\subsubsection*{Matrice Triangolare Superiore}
Una matrice quadrata si dice triangolare superiore se tutti gli elementi al di sotto della diagonale principale sono nulli ($a_{ij} = 0$ per $i>j$).
\paragraph{Notazione:}
\[ U = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
0 & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 0 & 9
\end{pmatrix} \]

\subsubsection*{Matrice Triangolare Inferiore}
Una matrice quadrata si dice triangolare inferiore se tutti gli elementi al di sopra della diagonale principale sono nulli ($a_{ij} = 0$ per $i<j$).
\paragraph{Notazione:}
\[ L = \begin{pmatrix}
a_{11} & 0 & \cdots & 0 \\
a_{21} & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 0 & 0 \\
4 & 5 & 0 \\
7 & 8 & 9
\end{pmatrix} \]

\subsubsection*{Matrice Diagonale}
Una matrice quadrata si dice diagonale se tutti gli elementi al di fuori della diagonale principale sono nulli ($a_{ij} = 0$ per $i \neq j$). È sia triangolare superiore che inferiore.
\paragraph{Notazione:}
\[ D = \begin{pmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 0 & 0 \\
0 & 5 & 0 \\
0 & 0 & 9
\end{pmatrix} \]

\subsubsection*{Matrice Simmetrica}
Una matrice quadrata si dice simmetrica se è uguale alla sua trasposta ($A = A^T$), ovvero se $a_{ij} = a_{ji}$ per ogni $i,j$.
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{12} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 2 & 3 \\
2 & 5 & 6 \\
3 & 6 & 9
\end{pmatrix} \]

\subsubsection*{Matrice Anti-simmetrica}
Una matrice quadrata si dice anti-simmetrica (o emisimmetrica) se è uguale all'opposto della sua trasposta ($A = -A^T$), ovvero se $a_{ij} = -a_{ji}$ per ogni $i,j$. Questo implica che gli elementi sulla diagonale principale sono nulli ($a_{ii} = 0$).
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
0 & a_{12} & \cdots & a_{1n} \\
-a_{12} & 0 & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
-a_{1n} & -a_{2n} & \cdots & 0
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
0 & 2 & -3 \\
-2 & 0 & 6 \\
3 & -6 & 0
\end{pmatrix} \]

\end{document}