\documentclass{article}

\usepackage{amsmath}
\usepackage[italian]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{xcolor}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

% This creates a new tcolorbox for definitions
\newtcbtheorem{definition}{Definizione} % {box name}{display name}
{
    colback=magenta!10!white,
    colframe=magenta!80!black,
    fonttitle=\bfseries
}
{def} % This is a prefix for the label, so you can reference it like \ref{def:vector_space}

\newtcbtheorem{theorem}{Teorema} % {box name}{display name}
{
    colback=blue!10!white,
    colframe=blue!80!black,
    fonttitle=\bfseries
}
{thm}

\newtcbtheorem{exercise}{Esercizio} % {box name}{display name}
{
    colback=green!10!white,
    colframe=green!80!black,
    fonttitle=\bfseries
}
{exc}

\title{Algebra Lineare}
\author{Federico Veronesi}

\begin{document}
\maketitle

{
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\section{Vettori}

\begin{definition}{Vettore}{vettore}
    Un vettore di campo $\mathbb{K}$ e dimensione $n$ è un insieme di elementi $a_1, a_2, \dots , a_n$, ciascuno di campo $\mathbb{K}$. In notazione: $\begin{pmatrix} x1 \\ \vdots \\ x_n \end{pmatrix}$ come ad esempio $\begin{pmatrix} 2 \\ -3.5 \\ \pi \end{pmatrix}$ di dimensione $3$ e campo $\mathbb{R}$
\end{definition}

\begin{definition}{Spazio Vettoriale}{spazio_vettoriale}
    Uno \textbf{spazio vettoriale} su un campo $\mathbb{K}$ è un insieme di \hyperref[def:vettore]{vettori} $V$ che definisce due operazioni:
    \begin{enumerate}
        \item Addizione fra vettori: $V + V \to V$
        \item Moltiplicazione scalare: $\mathbb{K} \times V \to V$
    \end{enumerate}
\end{definition}

Alcuni esempi di spazi vettoriali sono i seguenti:
\begin{itemize}
    \item $\mathbb{R} [x]$ L'insieme dei polinomi
    \item $\mathbb{R} [x]^{\le n}$ L'insieme dei polinomi di grado $\le n$
    \item $C(\mathbb{R}, \mathbb{R})$ Le funzioni continue da $\mathbb{R}$ in $\mathbb{R}$
    \item $M(m, n, \mathbb{K})$ L'insieme delle matrici $m \times n$ a coefficiente di campo $\mathbb{K}$
\end{itemize}

\subsection{Sottospazi Vettoriali}

\begin{definition}{Sottospazio Vettoriale}{sottospazio_vettoriale}
    Un sottospazio vettoriale è uno \hyperref[def:spazio_vettoriale]{spazio vettoriale} $W$ incluso in uno spazio vettoriale $V$. Moltiplicazione scalare di un vettore in $W$ e somma fra vettori di $W$ sono operazioni che ritornano un vettore incluso in $W$. Ne consegue che ogni sottospazio vettoriale include l'elemento neutro dello spazio vettoriale.
\end{definition}

\subsection{Combinazioni Lineari}

\begin{definition}{Combinazione Lineare}{combinazione_lineare}
    \begin{itemize}
        \item La combinazione lineare è il prodotto di un \hyperref[def:vettore]{vettore} per uno scalare diverso da zero.
        \item Le combinazioni lineari di un vettore sono l'insieme di tutti i vettori risultanti dal prodotto di quel vettore e uno scalare $\lambda$.
        \item Le combinazioni lineari di più vettori insieme sono l'insieme di tutte le somme vettoriali di tutte le combinazioni di combinazioni lineari di ciascuno dei vettori. In notazione: \it{guardare definizione di \hyperref[def:funzione_span]{SPAN}}
    \end{itemize}
\end{definition}

\begin{definition}{Funzione SPAN}{funzione_span}
    La funzione SPAN prende in ingresso un insieme di \hyperref[def:vettore]{vettori} e ritorna l'insieme delle loro combinazioni lineari.
    
    In notazione: \begin{itemize}
        \item \[SPAN(\begin{pmatrix}
            x_1 \\
            \vdots \\
            x_n
            \end{pmatrix}) = \{\begin{pmatrix}
            \lambda x_1 \\
            \vdots \\
            \lambda x_n
            \end{pmatrix} : \lambda \in \mathbb{R}\}\]
        \item \[SPAN(\begin{pmatrix}
            1 \\
            0
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            1
            \end{pmatrix}) = \{ \lambda\begin{pmatrix}
            1 \\
            0
            \end{pmatrix} + r\begin{pmatrix}
            0 \\
            1
            \end{pmatrix} : \lambda, r \in \mathbb{R}\}\]
    \end{itemize}

    Si possono permutare i vettori all'interno dell'insieme che viene passato a SPAN, visto che la combinazione lineare permette permutazione.
\end{definition}

\begin{definition}{Dipendenza Lineare}{dipendenza_lineare}
    Dei \hyperref[def:vettore]{vettori} sono linearmente indipendenti se nessuno di loro può essere espresso come \hyperref[def:combinazione_lineare]{combinazione lineare} degli altri.
\end{definition}

\begin{definition}{Base}{base}
    Ognuna delle basi di un insieme di \hyperref[def:vettore]{vettori} è un suo sottoinsieme di vettori \hyperref[def:dipendenza_lineare]{linearmente indipendenti} e la cui combinazione lineare genera l'insieme di partenza stesso

    "Da una base, con un vettore di coefficienti, posso ricavare qualsiasi vettore"
\end{definition}

\begin{definition}{Cardinalità}{cardinalita}
    Il numero di elementi di una base.
\end{definition}

\begin{definition}{Dimensione di Uno Spazio Vettoriale}{dimensione_spazio_vettoriale}
    La cardinalità di una qualunque base dello spazio vettoriale analizzato.
\end{definition}

\begin{exercise}{Basi}{}
    Dalla base $(\begin{pmatrix}
            1 \\
            2 \\
            3
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            1 \\
            4
            \end{pmatrix}, \begin{pmatrix}
            0 \\
            0 \\
            7
            \end{pmatrix})$ o da un'altra comunque simile inventata da te, con gli zero nei vettori non sostutuiti con numeri diversi da zero, ricava $\begin{pmatrix}
            4 \\
            6 \\
            9
            \end{pmatrix}$
\end{exercise}

\begin{exercise}{Basi}{}
    Dimostra che
    $(\begin{pmatrix}
        1 \\
        1 
    \end{pmatrix}, \begin{pmatrix}
        2 \\
        1 
    \end{pmatrix})$ è una base di $\mathbb(R)^2$
\end{exercise}

\begin{exercise}{Teorema Della Dimensione}{dimensione}
    Se uno spazio vettoriale $V$ ammette una base finita, allora tutte le sue basi hanno lo stesso numero di vettori, pari all'ordine dello spazio vettoriale.
\end{exercise}

\subsection{Trovare la Base di Vettori}

La combinazione lineare è invertibile perché:
\begin{itemize}
    \item A ogni vettore che è stato moltiplicato per uno scalare \lambda si può moltiplicare il reciproco di quello scalare $\lambda ^{-1}$ e tornare al vettore di partenza;
    \item A ogni vettore $v_1$ a cui è stato sommato un vettore $v_2$ si può sommare un vettore $v_2$ moltiplicato per uno scalare $-1$, cioè $-v_2$, per tornare al vettore di partenza.
\end{itemize}

Essendo \hyperref[def:funzione_span]{$SPAN$} invertibile, allora è possibile enunciare quanto segue. Presa \hyperref[def:funzione_span]{$SPAN$} come la funzione che parte da una base $e$ per arrivare a un insieme di vettori $V$, ed essendo \hyperref[def:funzione_span]{$SPAN$} invertibile, allora \hyperref[def:funzione_span]{$SPAN$} è anche la funzione che parte da $V$ per arrivare ad $e$. Quindi, preso un insieme di vettori, è possibile, grazie a \hyperref[def:funzione_span]{$SPAN$}, ricavare una delle basi che potrebbe generarlo.

Facciamo un esempio operativo di come applicare un algoritmo per ricavare una base di un insieme di vettori.

\[SPAN(\begin{pmatrix}
    1 \\
    0 \\
    1
\end{pmatrix}, \begin{pmatrix}
    2 \\
    1 \\
    1
\end{pmatrix}, \begin{pmatrix}
    -1 \\
    1 \\
    -1
\end{pmatrix})\]

Mettiamolo in forma di matrice:

\[\begin{pmatrix}
1 & 2 & -1 \\
0 & 1 & 1 \\
1 & 1 & -1
\end{pmatrix}\]

Moltiplichiamo per $-2$ temporaneamente il primo vettore, sommiamolo al secondo vettore, e moltiplichiamo per $\frac{1}{2}$ il primo vettore per farlo tornare al suo stato iniziale (questi passaggi intermedi che modificano temporaneamente un vettore ai fini di una operazione sono frequenti, infatti non verranno spiegati in futuro):

\[\begin{pmatrix}
1 & 0 & -1 \\
0 & 1 & 1 \\
1 & -1 & -1
\end{pmatrix}\]

Sommiamo al terzo vettore il primo:

\[\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1 \\
1 & -1 & 0
\end{pmatrix}\]

Sottraiamo (richiesto passaggio intermedio di moltiplicazione per $-1$, omesso) al secondo il terzo.

\[\begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 1 \\
1 & -1 & 0
\end{pmatrix}\]

Permutiamo (operazione di \hyperref[def:funzione_span]{$SPAN$} concessa) per avere uno schema a scalini

\[\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & -1
\end{pmatrix}\]

Come si può notare adesso si ha una \hyperref[def:matrice_triangolare_inferiore]{matrice triangolare inferiore}, e quindi si è sicuri che i vettori siano fra loro linearmente indipendenti, che è l'altra condizione necessaria perché la matrice ricavata sia una base. Questa si dice anche matrice a scalino, perché se gli zero fossero aria e i numeri il materiale di cui è fatta la scala, si avrebbe una scala. Non è condizione necessaria che la matrice ottenuta sia una matrice triangolare inferiore, l'importante è che sia a scalino

La dimostrazione che una matrice così fatta sia composta da vettori linearmente indipendenti la lasciamo al lettore come esercizio. (E' una battuta, non ci provare nemmeno a dimostrarlo)

\begin{exercise}{Ricavare una Base di Matrici in $\mathbb{R}^n$}{}
    Esecizi a piacere su matrici a piacere. Si ricorda che si cerca, idealmente, la base canonica, quindi partire con un numero di vettori pari al loro numero di elementi pari alla \hyperref[def:dimensione_spazio_vettoriale]{dimensione} di $\mathbb{R}$ utilizzata.
\end{exercise}

\section{Matrici}

\begin{definition}{Matrice}{matrice}
    Una matrice è un \hyperref[def:vettore]{vettore} bidimensionale $m \times n$. In notazione: $\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}$ Esempio di matrice di campo $\mathbb{N}$ e di dimensione $2 \times 3$: $\begin{pmatrix}
    1 & 4 \\
    3 & 7 \\
    2 & 13
    \end{pmatrix}$
\end{definition}

\begin{definition}{Traccia}{traccia}
    La traccia di una \hyperref[def:matrice]{matrice} è la somma degli elementi sulla sua diagonale principale. Ad esempio, la traccia di $\begin{pmatrix} 6 & 2 \\ 5 & 7 \end{pmatrix}$ è 13.
\end{definition}

\subsection{Tipologie di Matrici}

\begin{definition}{Matrice Quadrata}{matrice_quadrata}
Una matrice si dice quadrata se il numero di righe è uguale al numero di colonne ($m=n$).
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:} Matrice quadrata $3 \times 3$:
\[ \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{pmatrix} \]
\end{definition}

\begin{definition}{Matrice Triangolare Superiore}{matrice_triangolare_superiore}
Una matrice quadrata si dice triangolare superiore se tutti gli elementi al di sotto della diagonale principale sono nulli ($a_{ij} = 0$ per $i>j$).
\paragraph{Notazione:}
\[ U = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
0 & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 0 & 9
\end{pmatrix} \]
\end{definition}

\begin{definition}{Matrice Triangolare Inferiore}{matrice_triangolare_inferiore}
Una matrice quadrata si dice triangolare inferiore se tutti gli elementi al di sopra della diagonale principale sono nulli ($a_{ij} = 0$ per $i<j$).
\paragraph{Notazione:}
\[ L = \begin{pmatrix}
a_{11} & 0 & \cdots & 0 \\
a_{21} & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 0 & 0 \\
4 & 5 & 0 \\
7 & 8 & 9
\end{pmatrix} \]
\end{definition}

\begin{definition}{Matrice Diagonale}{matrice_diagonale}
Una matrice quadrata si dice diagonale se tutti gli elementi al di fuori della diagonale principale sono nulli ($a_{ij} = 0$ per $i \neq j$). È sia triangolare superiore che inferiore.
\paragraph{Notazione:}
\[ D = \begin{pmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 0 & 0 \\
0 & 5 & 0 \\
0 & 0 & 9
\end{pmatrix} \]
\end{definition}

\begin{definition}{Matrice Simmetrica}{matrice_simmetrica}
Una matrice quadrata si dice simmetrica se è uguale alla sua trasposta ($A = A^T$), ovvero se $a_{ij} = a_{ji}$ per ogni $i,j$.
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{12} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{nn}
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
1 & 2 & 3 \\
2 & 5 & 6 \\
3 & 6 & 9
\end{pmatrix} \]
\end{definition}

\begin{definition}{Matrice Anti-simmetrica}{matrice_anti_simmetrica}
Una matrice quadrata si dice anti-simmetrica (o emisimmetrica) se è uguale all'opposto della sua trasposta ($A = -A^T$), ovvero se $a_{ij} = -a_{ji}$ per ogni $i,j$. Questo implica che gli elementi sulla diagonale principale sono nulli ($a_{ii} = 0$).
\paragraph{Notazione:}
\[ A = \begin{pmatrix}
0 & a_{12} & \cdots & a_{1n} \\
-a_{12} & 0 & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
-a_{1n} & -a_{2n} & \cdots & 0
\end{pmatrix} \]
\paragraph{Esempio:}
\[ \begin{pmatrix}
0 & 2 & -3 \\
-2 & 0 & 6 \\
3 & -6 & 0
\end{pmatrix} \]
\end{definition}

\end{document}